# üíª AI Coding Assistant (RAG + LangChain + OpenAI)

## üìò Overview

This project is an **AI-powered coding assistant** built using **LangChain**, **OpenAI GPT**, and **FAISS**.
It uses **Retrieval-Augmented Generation (RAG)** to answer programming and debugging-related questions.

Instead of relying solely on the model‚Äôs internal knowledge, it retrieves relevant information from a **local knowledge base** ‚Äî containing documents like `debugging_guide.md` and `python_tips.txt` ‚Äî and combines it with GPT reasoning to produce accurate, context-aware answers.

---

## üß† Features

* üß© **Retrieval-Augmented Generation (RAG)** pipeline using LangChain
* üß† **Custom Knowledge Base**: stored locally in `data/knowledge_base/`
* üí¨ **ChatGPT-style Interface** built with Streamlit
* ‚öôÔ∏è **OpenAI API Integration** for high-quality responses
* üîí Secure configuration via `.env` file (API keys are never exposed)

---

## üß© Project Structure

```
rag_coding_assistant/
‚îÇ
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ ui_streamlit.py      # Streamlit-based chat UI
‚îÇ   ‚îú‚îÄ‚îÄ rag_chain.py         # Defines the RAG pipeline (Retriever + LLM)
‚îÇ   ‚îú‚îÄ‚îÄ vectorstore.py       # Builds and loads FAISS vector database
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ knowledge_base/
‚îÇ       ‚îú‚îÄ‚îÄ debugging_guide.md   # Troubleshooting & debugging tips
‚îÇ       ‚îú‚îÄ‚îÄ python_tips.txt      # Python tricks and common pitfalls
‚îÇ
‚îú‚îÄ‚îÄ .env                    # Stores API keys (not uploaded to GitHub)
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## üß∞ Tech Stack

| Component         | Technology         |
| ----------------- | ------------------ |
| Framework         | LangChain          |
| LLM Backend       | OpenAI GPT-4o-mini |
| Vector Store      | FAISS              |
| UI                | Streamlit          |
| Environment       | Python 3.10+       |
| Config Management | python-dotenv      |

---

## ‚öôÔ∏è Setup Instructions

### 1Ô∏è‚É£ Clone the Repository

```bash
https://github.com/Geetha-Mahadevappa/llm-chatbot.git
cd llm-chatbot
```

### 2Ô∏è‚É£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 3Ô∏è‚É£ Set Up Environment Variables

Create a `.env` file in your project root (you can start by copying `.env.example`):

```bash
cp .env.example .env
OPENAI_API_KEY=sk-your-openai-api-key
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=your-langsmith-api-key
LANGCHAIN_PROJECT=llm-chatbot
```

> ‚ö†Ô∏è Never commit your `.env` file to GitHub.

---

## üß† Knowledge Base

The folder `data/knowledge_base/` contains reference material for the chatbot to retrieve answers from.

| File                 | Purpose                                                                     |
| -------------------- | --------------------------------------------------------------------------- |
| `debugging_guide.md` | Contains practical tips for debugging Python code.                          |
| `python_tips.txt`    | Includes common Python coding tricks, best practices, and syntax reminders. |

You can **add more `.txt` or `.md` files** here to expand your assistant‚Äôs knowledge.

---

## üß© Script Descriptions

### `app/vectorstore.py`

* Loads all text/markdown files from `data/knowledge_base/`
* Splits documents into smaller chunks for embedding
* Uses `OpenAIEmbeddings()` to create vector representations
* Stores embeddings locally in a **FAISS** index
* Run this once to build the vectorstore:

  ```bash
  python -m app.vectorstore
  ```

---

### `app/rag_chain.py`

* Loads the FAISS vectorstore and wraps it in a retriever
* Initializes the OpenAI LLM (`gpt-4o-mini`)
* Combines retrieval and generation via `RetrievalQA`
* Acts as the **core logic** for the chatbot‚Äôs reasoning

---

### `app/ui_streamlit.py`

* Provides an **interactive web UI** (similar to ChatGPT)
* Maintains chat history using Streamlit session state
* Sends user queries to the RAG chain and displays answers
* Run this to start the chatbot:

  ```bash
  streamlit run app/ui_streamlit.py
  ```

### `app/cli.py`

* Lightweight command-line interface for quick questions
* Supports adjusting retrieval depth and model temperature
* Ask a single question:

  ```bash
  python -m app.cli "How do I reverse a string in Python?"
  ```

* Start an interactive prompt:

  ```bash
  python -m app.cli
  ```

---

## üß© How It Works

1. You type a coding or debugging question in the chat.
2. The app retrieves relevant text chunks from your `knowledge_base` using FAISS.
3. The retrieved context is passed to the OpenAI GPT model.
4. The model generates an accurate, context-aware response.

```
User Question
     ‚Üì
Vectorstore Retriever (FAISS)
     ‚Üì
LangChain RAG (Retriever + LLM)
     ‚Üì
OpenAI GPT Response
     ‚Üì
Streamlit Chat UI
```

---

## üß™ Example Queries

You can try:

```
> How do I fix a syntax error in Python?
> What are some debugging best practices?
> Show me how to create a Python function that reverses a string.
> How do I handle exceptions in Python?
```

---

## üîí Environment Variables

| Variable               | Description                                  |
| ---------------------- | -------------------------------------------- |
| `OPENAI_API_KEY`       | Your OpenAI API key                          |
| `LANGCHAIN_API_KEY`    | API key for LangSmith (optional for tracing) |
| `LANGCHAIN_TRACING_V2` | Enable tracing of LangChain runs             |
| `LANGCHAIN_PROJECT`    | Project name for LangSmith dashboard         |

---

Would you like me to generate a **short GitHub ‚ÄúAbout‚Äù description (1‚Äì2 lines)** for your repo page ‚Äî the one that appears at the top under your project tit
